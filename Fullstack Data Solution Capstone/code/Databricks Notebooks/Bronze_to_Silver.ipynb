{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "e0c48c81-ca76-4941-aa88-cb7220b418fd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install missingno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8d3e5547-0ddd-4207-908b-44f95330bb9c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "import datetime\n",
    "import requests\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import missingno as msno\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e4272ceb-a76d-4e20-8a77-01eed7e8abf2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Databricks notebook source\n",
    "# Unmounting\n",
    "\n",
    "mount_point = \"/mnt/capstone\"\n",
    "if any(m.mountPoint == mount_point for m in dbutils.fs.mounts()):\n",
    "    dbutils.fs.unmount(mount_point)\n",
    "\n",
    "storage_account_name = \"jscapstone\"\n",
    "container_name = \"capstone\"\n",
    "sas_token = \"sv=2024-11-04&ss=bfqt&srt=co&sp=rwdlacupyx&se=2026-11-05T06:48:55Z&st=2025-11-04T22:33:55Z&spr=https&sig=C9I%2BFRRtANmD1KDpe%2BthyWBuhsjgVQXRe2%2BwtFr0X6s%3D\"  \n",
    "\n",
    "# Mount if not already mounted\n",
    "if not any(m.mountPoint == mount_point for m in dbutils.fs.mounts()):\n",
    "    dbutils.fs.mount(\n",
    "        source = f\"wasbs://{container_name}@{storage_account_name}.blob.core.windows.net/\",\n",
    "        mount_point = mount_point,\n",
    "        extra_configs = {f\"fs.azure.sas.{container_name}.{storage_account_name}.blob.core.windows.net\": sas_token}\n",
    "    )\n",
    "\n",
    "# List files to confirm\n",
    "display(dbutils.fs.ls(\"/mnt/capstone\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9474d356-75a3-4923-8599-e738db26d86a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Load CSV Files\n",
    "\n",
    "google_media_path = '/mnt/capstone/bronze/google/'\n",
    "meta_media_path = '/mnt/capstone/bronze/meta/'\n",
    "internal_media_path = '/mnt/capstone/bronze/internal/'\n",
    "# Google Media\n",
    "google_media_df = spark.read.csv(google_media_path, header=True, inferSchema=True)\n",
    "display(google_media_df)\n",
    "# Meta Media\n",
    "meta_media_df = spark.read.csv(meta_media_path, header=True, inferSchema=True)\n",
    "display(meta_media_df)\n",
    "# Internal Media\n",
    "internal_df = spark.read.csv(internal_media_path, header=True, inferSchema=True)\n",
    "display(internal_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b5571843-3601-4ce1-bfe4-183778cef7a7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Checking Lengths \n",
    "print(f'Google DF length: {google_media_df.count()}')\n",
    "print(f'Meta DF length: {meta_media_df.count()}')\n",
    "print(f'Internal DF length: {internal_df.count()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "60959254-3839-40a8-9059-194b83f69005",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Change all columns to lowercase\n",
    "def lowercase_columns(df):\n",
    "    for old_col in df.columns:\n",
    "        new_col = old_col.lower()\n",
    "        df = df.withColumnRenamed(old_col, new_col)\n",
    "    return df\n",
    "\n",
    "google_media_df = lowercase_columns(google_media_df)\n",
    "meta_media_df = lowercase_columns(meta_media_df)\n",
    "internal_df = lowercase_columns(internal_df)\n",
    "\n",
    "google_media_df.printSchema()\n",
    "meta_media_df.printSchema()\n",
    "internal_df.printSchema()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3b68a9ab-70cf-475c-b2c1-72ba9511a6a8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Change Column Names for Readability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a3c8f75e-8f6d-42c9-b1ac-1ec842770641",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Convert 'organisation_id' to 'organization_id'\n",
    "google_media_df = google_media_df.withColumnRenamed('organisation_id', 'organization_id')\n",
    "meta_media_df = meta_media_df.withColumnRenamed('organisation_id', 'organization_id')\n",
    "internal_df = internal_df.withColumnRenamed('organisation_id', 'organization_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e717b721-3adb-43a5-a9b1-5266a6d640f6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "internal_df = internal_df.withColumnRenamed('first_purchases', 'new_customers')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "db33a20d-9357-42de-aaa1-d4790cdf75d2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Check Nulls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "61ab0e73-fbf8-4dda-9ba8-8ef3ea266a39",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Convert Spark df to Pandas DF \n",
    "google_pd_df = google_media_df.toPandas()\n",
    "meta_pd_df = meta_media_df.toPandas()\n",
    "internal_pd_df = internal_df.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7db1739b-edf6-412c-9a17-e050efea4272",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "There are null values in the spend, clicks, and metrics media. Let's use missingno to check missing at random."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d2d45ef7-2841-4ee3-894e-e2600c09ccd1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Calculate percentage of rows with missing values to see if we can delete all the rows\n",
    "# for loop to generate percentage of missing values\n",
    "\n",
    "def percent_missing(df):\n",
    "    percent_rows_na = df.isnull().any(axis=1).sum() \n",
    "    perc = percent_rows_na / len(df) * 100\n",
    "    print(f'Percentage of row with missing values against dataframe total rows: {perc}') \n",
    "\n",
    "for df in [google_pd_df, meta_pd_df, internal_pd_df]:\n",
    "    percent_missing(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a654150e-f3d2-4902-8666-6a38c7ef35bb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "There is alot of missing rows in google media and meta media - check if these are missing at random or not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "59210ba0-7529-4071-819e-75b2330f031f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Visualize Missingness with missingno Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "65727f4e-a033-43ff-8b53-f6a73a42a9cd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Sort by date_day\n",
    "google_order = ['organization_id', 'date_day', 'google_paid_search_spend', 'google_paid_search_impressions', 'google_paid_search_clicks', \n",
    "    'google_shopping_spend', 'google_shopping_impressions', 'google_shopping_clicks', 'google_pmax_spend', 'google_pmax_impressions', 'google_pmax_clicks']\n",
    "\n",
    "meta_order = ['organization_id', 'date_day', 'meta_facebook_spend', 'meta_facebook_impressions', 'meta_facebook_clicks', 'meta_instagram_spend', 'meta_instagram_impressions', 'meta_instagram_clicks']\n",
    "\n",
    "msno.matrix(google_pd_df[google_order])\n",
    "plt.title(\"Missing Data Matrix for Google Media Dataset\")\n",
    "\n",
    "msno.matrix(meta_pd_df[meta_order])\n",
    "plt.title(\"Missing Data Matrix for Meta Media Dataset\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "889338c7-1e7c-4d5f-b70f-e40dccde8648",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "The missing values are missing not at random and as suspected are date specific, based no non-media spend dates. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b6959911-392d-4561-97a3-20899ddb5579",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "751772a4-ddb4-48dd-aaaa-be4a6560b1f0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mKeyError\u001B[0m                                  Traceback (most recent call last)\n",
       "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/pandas/core/indexes/base.py:3802\u001B[0m, in \u001B[0;36mIndex.get_loc\u001B[0;34m(self, key, method, tolerance)\u001B[0m\n",
       "\u001B[1;32m   3801\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n",
       "\u001B[0;32m-> 3802\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_engine\u001B[38;5;241m.\u001B[39mget_loc(casted_key)\n",
       "\u001B[1;32m   3803\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mKeyError\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m err:\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/pandas/_libs/index.pyx:138\u001B[0m, in \u001B[0;36mpandas._libs.index.IndexEngine.get_loc\u001B[0;34m()\u001B[0m\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/pandas/_libs/index.pyx:165\u001B[0m, in \u001B[0;36mpandas._libs.index.IndexEngine.get_loc\u001B[0;34m()\u001B[0m\n",
       "\n",
       "File \u001B[0;32mpandas/_libs/hashtable_class_helper.pxi:5745\u001B[0m, in \u001B[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001B[0;34m()\u001B[0m\n",
       "\n",
       "File \u001B[0;32mpandas/_libs/hashtable_class_helper.pxi:5753\u001B[0m, in \u001B[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001B[0;34m()\u001B[0m\n",
       "\n",
       "\u001B[0;31mKeyError\u001B[0m: 'no_spend_day'\n",
       "\n",
       "The above exception was the direct cause of the following exception:\n",
       "\n",
       "\u001B[0;31mKeyError\u001B[0m                                  Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-6174930250428085>, line 48\u001B[0m\n",
       "\u001B[1;32m     43\u001B[0m         \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mcol\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mzero_count\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m days (\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mzero_count\u001B[38;5;250m \u001B[39m\u001B[38;5;241m/\u001B[39m\u001B[38;5;250m \u001B[39m\u001B[38;5;28mlen\u001B[39m(media_pd_df)\u001B[38;5;250m \u001B[39m\u001B[38;5;241m*\u001B[39m\u001B[38;5;250m \u001B[39m\u001B[38;5;241m100\u001B[39m\u001B[38;5;132;01m:\u001B[39;00m\u001B[38;5;124m.2f\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m%)\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
       "\u001B[1;32m     47\u001B[0m \u001B[38;5;66;03m# Filter for zero-spend days where new_customers > 0\u001B[39;00m\n",
       "\u001B[0;32m---> 48\u001B[0m zero_spend_with_customers \u001B[38;5;241m=\u001B[39m df[(df[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mno_spend_day\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m1\u001B[39m) \u001B[38;5;241m&\u001B[39m (df[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mnew_customers\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m)]\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/pandas/core/frame.py:3807\u001B[0m, in \u001B[0;36mDataFrame.__getitem__\u001B[0;34m(self, key)\u001B[0m\n",
       "\u001B[1;32m   3805\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcolumns\u001B[38;5;241m.\u001B[39mnlevels \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m1\u001B[39m:\n",
       "\u001B[1;32m   3806\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_getitem_multilevel(key)\n",
       "\u001B[0;32m-> 3807\u001B[0m indexer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcolumns\u001B[38;5;241m.\u001B[39mget_loc(key)\n",
       "\u001B[1;32m   3808\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m is_integer(indexer):\n",
       "\u001B[1;32m   3809\u001B[0m     indexer \u001B[38;5;241m=\u001B[39m [indexer]\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/pandas/core/indexes/base.py:3804\u001B[0m, in \u001B[0;36mIndex.get_loc\u001B[0;34m(self, key, method, tolerance)\u001B[0m\n",
       "\u001B[1;32m   3802\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_engine\u001B[38;5;241m.\u001B[39mget_loc(casted_key)\n",
       "\u001B[1;32m   3803\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mKeyError\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m err:\n",
       "\u001B[0;32m-> 3804\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mKeyError\u001B[39;00m(key) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01merr\u001B[39;00m\n",
       "\u001B[1;32m   3805\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m:\n",
       "\u001B[1;32m   3806\u001B[0m     \u001B[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001B[39;00m\n",
       "\u001B[1;32m   3807\u001B[0m     \u001B[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001B[39;00m\n",
       "\u001B[1;32m   3808\u001B[0m     \u001B[38;5;66;03m#  the TypeError.\u001B[39;00m\n",
       "\u001B[1;32m   3809\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_check_indexing_error(key)\n",
       "\n",
       "\u001B[0;31mKeyError\u001B[0m: 'no_spend_day'"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": {
        "ename": "KeyError",
        "evalue": "'no_spend_day'"
       },
       "metadata": {
        "errorSummary": ""
       },
       "removedWidgets": [],
       "sqlProps": null,
       "stackFrames": [
        "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
        "\u001B[0;31mKeyError\u001B[0m                                  Traceback (most recent call last)",
        "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/pandas/core/indexes/base.py:3802\u001B[0m, in \u001B[0;36mIndex.get_loc\u001B[0;34m(self, key, method, tolerance)\u001B[0m\n\u001B[1;32m   3801\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m-> 3802\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_engine\u001B[38;5;241m.\u001B[39mget_loc(casted_key)\n\u001B[1;32m   3803\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mKeyError\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m err:\n",
        "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/pandas/_libs/index.pyx:138\u001B[0m, in \u001B[0;36mpandas._libs.index.IndexEngine.get_loc\u001B[0;34m()\u001B[0m\n",
        "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/pandas/_libs/index.pyx:165\u001B[0m, in \u001B[0;36mpandas._libs.index.IndexEngine.get_loc\u001B[0;34m()\u001B[0m\n",
        "File \u001B[0;32mpandas/_libs/hashtable_class_helper.pxi:5745\u001B[0m, in \u001B[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001B[0;34m()\u001B[0m\n",
        "File \u001B[0;32mpandas/_libs/hashtable_class_helper.pxi:5753\u001B[0m, in \u001B[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001B[0;34m()\u001B[0m\n",
        "\u001B[0;31mKeyError\u001B[0m: 'no_spend_day'",
        "\nThe above exception was the direct cause of the following exception:\n",
        "\u001B[0;31mKeyError\u001B[0m                                  Traceback (most recent call last)",
        "File \u001B[0;32m<command-6174930250428085>, line 48\u001B[0m\n\u001B[1;32m     43\u001B[0m         \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mcol\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mzero_count\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m days (\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mzero_count\u001B[38;5;250m \u001B[39m\u001B[38;5;241m/\u001B[39m\u001B[38;5;250m \u001B[39m\u001B[38;5;28mlen\u001B[39m(media_pd_df)\u001B[38;5;250m \u001B[39m\u001B[38;5;241m*\u001B[39m\u001B[38;5;250m \u001B[39m\u001B[38;5;241m100\u001B[39m\u001B[38;5;132;01m:\u001B[39;00m\u001B[38;5;124m.2f\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m%)\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     47\u001B[0m \u001B[38;5;66;03m# Filter for zero-spend days where new_customers > 0\u001B[39;00m\n\u001B[0;32m---> 48\u001B[0m zero_spend_with_customers \u001B[38;5;241m=\u001B[39m df[(df[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mno_spend_day\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m1\u001B[39m) \u001B[38;5;241m&\u001B[39m (df[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mnew_customers\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m)]\n",
        "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/pandas/core/frame.py:3807\u001B[0m, in \u001B[0;36mDataFrame.__getitem__\u001B[0;34m(self, key)\u001B[0m\n\u001B[1;32m   3805\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcolumns\u001B[38;5;241m.\u001B[39mnlevels \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m1\u001B[39m:\n\u001B[1;32m   3806\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_getitem_multilevel(key)\n\u001B[0;32m-> 3807\u001B[0m indexer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcolumns\u001B[38;5;241m.\u001B[39mget_loc(key)\n\u001B[1;32m   3808\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m is_integer(indexer):\n\u001B[1;32m   3809\u001B[0m     indexer \u001B[38;5;241m=\u001B[39m [indexer]\n",
        "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/pandas/core/indexes/base.py:3804\u001B[0m, in \u001B[0;36mIndex.get_loc\u001B[0;34m(self, key, method, tolerance)\u001B[0m\n\u001B[1;32m   3802\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_engine\u001B[38;5;241m.\u001B[39mget_loc(casted_key)\n\u001B[1;32m   3803\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mKeyError\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m err:\n\u001B[0;32m-> 3804\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mKeyError\u001B[39;00m(key) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01merr\u001B[39;00m\n\u001B[1;32m   3805\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m:\n\u001B[1;32m   3806\u001B[0m     \u001B[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001B[39;00m\n\u001B[1;32m   3807\u001B[0m     \u001B[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001B[39;00m\n\u001B[1;32m   3808\u001B[0m     \u001B[38;5;66;03m#  the TypeError.\u001B[39;00m\n\u001B[1;32m   3809\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_check_indexing_error(key)\n",
        "\u001B[0;31mKeyError\u001B[0m: 'no_spend_day'"
       ],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# define media\n",
    "google_cols = [\n",
    "    'google_paid_search_spend','google_paid_search_impressions', 'google_paid_search_clicks',\n",
    "    'google_shopping_spend', 'google_shopping_impressions', 'google_shopping_clicks',\n",
    "    'google_pmax_spend', 'google_pmax_impressions', 'google_pmax_clicks',\n",
    "    'google_display_spend', 'google_display_impressions', 'google_display_clicks',\n",
    "    'google_video_spend', 'google_video_impressions', 'google_video_clicks',\n",
    "    'meta_facebook_spend', 'meta_facebook_impressions', 'meta_facebook_clicks',\n",
    "    'meta_instagram_spend', 'meta_instagram_impressions', 'meta_instagram_clicks',\n",
    "    'meta_other_spend', 'meta_other_impressions', 'meta_other_clicks'\n",
    "]\n",
    "\n",
    "# merge meta and google on organization_id and date_day\n",
    "media_pd_df = google_pd_df.merge(meta_pd_df, on=['organization_id', 'date_day'])\n",
    "media_pd_df = media_pd_df.sort_values(by=['organization_id', 'date_day'])\n",
    "\n",
    "# Create zero flags for each channel individually\n",
    "for col in media_cols:\n",
    "     media_pd_df[f'{col}_zero_flag'] = (media_pd_df[col] == 0).astype(int)\n",
    "\n",
    "# Print zero-spend, zero-clicks, zero-impressions per channel\n",
    "\n",
    "print(\"Zero-spend days per channel:\")\n",
    "for col in media_cols:\n",
    "\n",
    "    # if column name has has 'spend' in the name, then do calculation\n",
    "    if 'spend' in col:\n",
    "        zero_count = media_pd_df[f'{col}_zero_flag'].sum()\n",
    "        print(f\"{col}: {zero_count} days ({zero_count / len(media_pd_df) * 100:.2f}%)\")\n",
    "\n",
    "print(\"Zero-impressions days per channel:\")\n",
    "for col in media_cols:\n",
    "    # if column *_zero_flag has 'impressions' in the name, then do calculation\n",
    "    if 'impressions' in col:\n",
    "        zero_count = media_pd_df[f'{col}_zero_flag'].sum()\n",
    "        print(f\"{col}: {zero_count} days ({zero_count / len(media_pd_df) * 100:.2f}%)\")\n",
    "\n",
    "print(\"Zero-clicks days per channel:\")\n",
    "for col in media_cols:\n",
    "    # if column *_zero_flag has 'clicks' in the name, then do calculation\n",
    "    if 'clicks' in col:\n",
    "        zero_count = media_pd_df[f'{col}_zero_flag'].sum()\n",
    "        print(f\"{col}: {zero_count} days ({zero_count / len(media_pd_df) * 100:.2f}%)\")\n",
    "\n",
    "\n",
    "\n",
    "# Filter for zero-spend days where new_customers > 0\n",
    "zero_spend_with_customers = df[(df['no_spend_day'] == 1) & (df['new_customers'] > 0)]\n",
    "\n",
    "# # Show counts and proportions\n",
    "# total_zero_spend = df['no_spend_day'].sum()\n",
    "# rows_with_customers = len(zero_spend_with_customers)\n",
    "\n",
    "# print(\"\\nOverall zero-spend days across all channels:\")\n",
    "# print(f\"Zero-spend days with new customers: {rows_with_customers}\")\n",
    "# print(f\"Total zero-spend days: {total_zero_spend}\")\n",
    "# print(f\"Percentage of zero-spend days with customers: {rows_with_customers / total_zero_spend:.2%}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7edc0f2e-3995-4cae-bb44-4ec5bdc88ca3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Go back to Spark df: Fill in missing columns in google_media_df with 0 for no-spend and inactive media days \n",
    "google_media_df = google_media_df.na.fill(0)\n",
    "\n",
    "# Fill in missing in meta_media_df with 0 for no-spend and inactive media days\n",
    "meta_media_df = meta_media_df.na.fill(0)\n",
    "\n",
    "print(f\"Rows: {google_media_df.count()}, Columns: {len(google_media_df.columns)}\")\n",
    "print(f\"Rows: {google_media_df.count()}, Columns: {len(google_media_df.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9db0c3e0-dadc-4a7e-95c5-d637cf8ea74c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Check for Duplicates\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d266cfac-4711-40b7-9793-3cb9b1dd87e3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Check duplicates using pandas dataframe\n",
    "\n",
    "display(google_pd_df.duplicated().value_counts())\n",
    "display(meta_pd_df.duplicated().value_counts())\n",
    "display(internal_pd_df.duplicated().value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "11940c82-6cf5-4045-8c01-5e1571cc4a2b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Fetch Holiday Data from Nager API using internal file min and max dates and store raw JSON load into bronze folder\n",
    "# Get date range from internal data\n",
    "min_date = internal_df.agg({\"date_day\": \"min\"}).collect()[0][0]\n",
    "max_date = internal_df.agg({\"date_day\": \"max\"}).collect()[0][0]\n",
    "\n",
    "start_year = min_date.year\n",
    "end_year = max_date.year\n",
    "\n",
    "# Set container for holidays so we don't get duplicates\n",
    "fetched_holidays = set()\n",
    "\n",
    "excluded_holidays = [\"Good Friday\", \"Lincoln's Birthday\", \"Truman Day\"]\n",
    "\n",
    "for year in range(start_year, end_year + 1):\n",
    "    try:\n",
    "        url = f'https://date.nager.at/api/v3/PublicHolidays/{year}/US'\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()\n",
    "        holidays = response.json()\n",
    "\n",
    "        for h in holidays:\n",
    "            if h[\"name\"] not in excluded_holidays:\n",
    "                fetched_holidays.add(datetime.datetime.strptime(h['date'], \"%Y-%m-%d\").date())\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error fetching data for {year}: {e}\")\n",
    "    \n",
    "    # Print completed fetch request and year of holiday\n",
    "    print(f'Completed holiday fetch request for {year}')\n",
    "\n",
    "# Save holiday array as JSON to bronze folder\n",
    "holiday_path = \"/dbfs/mnt/capstone/bronze/nager_api/public_holidays.json\"\n",
    "with open(holiday_path, 'w') as f:\n",
    "    json.dump(holiday_array, f)\n",
    "\n",
    "# Sort list\n",
    "fetched_holidays = sorted(fetched_holidays)\n",
    "\n",
    "print(fetched_holidays)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e1f257e4-74cc-45a3-bf0d-b778934f6351",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Save cleaned holidays data as JSON file for silver stage\n",
    "cleaned_holiday_path = \"/dbfs/mnt/capstone/silver/holiday_cleaned/cleaned_public_holidays.json\"\n",
    "with open(cleaned_holiday_path, \"w\") as f:\n",
    "    # Write each holiday date as a separate line in the file for reading in spark in later stages\n",
    "     for d in fetched_holidays:\n",
    "        json.dump({\"holiday_date\": d.isoformat()}, f)\n",
    "        f.write(\"\\n\")\n",
    "\n",
    "print(f\"Saved cleaned holidays to {cleaned_holiday_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "10518590-7dcc-49e4-aced-7686f1ae8053",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Save cleaned data as csv in silver folder\n",
    "\n",
    "# Array must stay in same order since they make pairs using zip\n",
    "silver_files = ['google', 'meta', 'internal']\n",
    "df_list = [google_media_df, meta_media_df, internal_df]\n",
    "\n",
    "for df_name, endpoint in zip(df_list, silver_files):\n",
    "    path_name = f'/mnt/capstone/silver/{endpoint}'\n",
    "    df_name.write.format('delta').mode('overwrite').save(path_name)\n",
    "\n",
    "    print(f'Saved cleaned {endpoint} data to {path_name}')\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Bronze_to_Silver",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}